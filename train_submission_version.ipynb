{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "# Use ensemble to train the model\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "import sys\n",
    "\n",
    "# In the second part of the notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# This part of importing would be moved to main.py or train.py later\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a hyperparameter class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.objective = 'regression'\n",
    "        self.num_leaves = 31\n",
    "        self.learning_rate = 0.01\n",
    "        self.n_estimators = 120\n",
    "        self.max_bin = 255\n",
    "        self.bagging_fraction = 0.8\n",
    "        self.bagging_freq = 5\n",
    "        self.feature_fraction = 0.8\n",
    "        self.feature_fraction_seed = 9\n",
    "        self.bagging_seed = 9\n",
    "        self.random_state = 42\n",
    "        self.OrgFertilizers = 'None'\n",
    "        self.FirstTopDressFert = 'None' \n",
    "        self.CropbasalFerts = 'None'\n",
    "        self.NursDetFactor = 'None'\n",
    "        self.LandPreparationMethod = 'None'\n",
    "        self.TransDetFactor = 'None'\n",
    "        self.PCropSolidOrgFertAppMethod = 'None'\n",
    "        self.sparse_threshold = 1.0\n",
    "# Instantiate the Hyperparameters class\n",
    "config = Config()\n",
    "\n",
    "# Initialize a wandb run\n",
    "wandb.init(project='zindi-crop-challenge',\n",
    "           entity=\"lmu-seminar\")\n",
    "\n",
    "# Update the wandb configuration with the hyperparameters\n",
    "wandb.config.update(vars(config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "path = 'Train.csv'\n",
    "dataset = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'Train.csv'\n",
    "dataset_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'Yield' column\n",
    "train_labels = dataset['Yield'].copy()\n",
    "dataset = dataset.drop(columns=['ID','Yield'], axis = 1)\n",
    "dataset_test = dataset_test.drop(columns=['ID'], axis = 1)\n",
    "\n",
    "# Copy the original dataset\n",
    "dataset_original = dataset.copy()\n",
    "dataset_test_original = dataset_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Harv_hand_rent'] = dataset['Harv_hand_rent'].fillna(0)\n",
    "dataset['OrgFertilizers'] = dataset['OrgFertilizers'].fillna(dataset['OrgFertilizers'].mode()[0])\n",
    "dataset['CropbasalFerts'] = dataset['CropbasalFerts'].fillna(dataset['CropbasalFerts'].mode()[0])\n",
    "dataset['FirstTopDressFert'] = dataset['FirstTopDressFert'].fillna(dataset['FirstTopDressFert'].mode()[0])\n",
    "dataset['NursDetFactor'] = dataset['NursDetFactor'].fillna(dataset['NursDetFactor'].mode()[0])\n",
    "dataset['LandPreparationMethod'] = dataset['LandPreparationMethod'].fillna(dataset['LandPreparationMethod'].mode()[0])\n",
    "dataset['TransDetFactor'] = dataset['TransDetFactor'].fillna(dataset['TransDetFactor'].mode()[0])\n",
    "dataset['PCropSolidOrgFertAppMethod'] = dataset['PCropSolidOrgFertAppMethod'].fillna(dataset['PCropSolidOrgFertAppMethod'].mode()[0])\n",
    "\n",
    "dataset_test['Harv_hand_rent'] = dataset_test['Harv_hand_rent'].fillna(0)\n",
    "dataset_test['OrgFertilizers'] = dataset_test['OrgFertilizers'].fillna(dataset_test['OrgFertilizers'].mode()[0])\n",
    "dataset_test['CropbasalFerts'] = dataset_test['CropbasalFerts'].fillna(dataset_test['CropbasalFerts'].mode()[0])\n",
    "dataset_test['FirstTopDressFert'] = dataset_test['FirstTopDressFert'].fillna(dataset_test['FirstTopDressFert'].mode()[0])\n",
    "dataset_test['NursDetFactor'] = dataset_test['NursDetFactor'].fillna(dataset_test['NursDetFactor'].mode()[0])\n",
    "dataset_test['LandPreparationMethod'] = dataset_test['LandPreparationMethod'].fillna(dataset_test['LandPreparationMethod'].mode()[0])\n",
    "dataset_test['TransDetFactor'] = dataset_test['TransDetFactor'].fillna(dataset_test['TransDetFactor'].mode()[0])\n",
    "dataset_test['PCropSolidOrgFertAppMethod'] = dataset_test['PCropSolidOrgFertAppMethod'].fillna(dataset_test['PCropSolidOrgFertAppMethod'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique values for categorical variables to identify sparse classes\n",
    "categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
    "sparse_classes = {col: dataset[col].nunique() for col in categorical_columns if dataset[col].nunique() > 15} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['LandPreparationMethod', 'CropTillageDate', 'RcNursEstDate', 'SeedingSowingTransplanting', 'NursDetFactor', 'TransDetFactor', 'OrgFertilizers', 'CropbasalFerts', 'Harv_date', 'Threshing_date'])\n"
     ]
    }
   ],
   "source": [
    "# check the name of sparse classes\n",
    "print(sparse_classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([43, 78, 61, 62, 125, 155, 31, 34, 107, 162])\n"
     ]
    }
   ],
   "source": [
    "# check the unique values of sparse classes\n",
    "print(sparse_classes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for grouping\n",
    "threshold_percentage = 1\n",
    "threshold = len(dataset) * (threshold_percentage / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group sparse classes\n",
    "def group_sparse_classes(df, column, threshold):\n",
    "    # Find the categories that are below the threshold\n",
    "    value_counts = df[column].value_counts()\n",
    "    to_replace = value_counts[value_counts <= threshold].index.tolist()\n",
    "    \n",
    "    # Replace the sparse classes with 'other'\n",
    "    df[column] = df[column].replace(to_replace, 'other')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grouping for the identified categorical variables with many unique values\n",
    "for col in ['LandPreparationMethod', 'OrgFertilizers', 'CropbasalFerts']:\n",
    "    dataset = group_sparse_classes(dataset, col, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the category adjustment to the test dataset based on the training dataset categories\n",
    "for col in ['LandPreparationMethod', 'OrgFertilizers', 'CropbasalFerts']:\n",
    "    test_dataset = group_sparse_classes(dataset_test, col, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 skewed numerical features to Box Cox transform\n",
      "CultLand                         8.178796\n",
      "CropCultLand                     8.796317\n",
      "SeedlingsPerPit                 53.528364\n",
      "TransplantingIrrigationHours    30.754264\n",
      "TransIrriCost                    3.912326\n",
      "StandingWater                    1.848641\n",
      "Ganaura                          5.237242\n",
      "CropOrgFYM                       6.876849\n",
      "BasalDAP                         2.905715\n",
      "BasalUrea                        2.244465\n",
      "1tdUrea                          2.094479\n",
      "1appDaysUrea                     4.769872\n",
      "2tdUrea                          2.696580\n",
      "2appDaysUrea                    -1.589283\n",
      "Harv_hand_rent                  40.547049\n",
      "Residue_length                  -1.880371\n",
      "Residue_perc                     3.747956\n",
      "Acre                             2.384037\n",
      "dtype: float64\n",
      "There are 11 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "# fill the high skewness columns with median\n",
    "# check the skewness of the numerical columns\n",
    "skewness = dataset.skew(numeric_only=True)\n",
    "skewness = skewness[abs(skewness) > 0.5]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "# sort the skewness in descending order\n",
    "skewness.sort_values(ascending=False)\n",
    "print(skewness)\n",
    "\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "# apply the boxcox1p transformation to the skewed features\n",
    "for feat in skewed_features:\n",
    "    dataset[feat] = boxcox1p(dataset[feat], lam)\n",
    "    dataset_test[feat] = boxcox1p(dataset_test[feat], lam)\n",
    "\n",
    "# check the skewness of the numerical columns\n",
    "skewness = dataset.skew(numeric_only=True)\n",
    "skewness = skewness[abs(skewness) > 0.5]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_skewed_features = ['SeedingsPerpit', 'TransplantingIrrigationHours','Harv_hand_rent']\n",
    "low_skewed_features = ['CultLand', 'CropOrgFYM', 'CropCultLand','Ganaura'] \n",
    "moderate_skewed_features = ['BasalUrea', 'BasalDAP',\n",
    "                            '1tdUrea', '1appDaysUrea', \n",
    "                            '2tdUrea', '2appDaysUrea',\n",
    "                            'Residue_length', 'Residue_perc',\n",
    "                            'StandingWater', 'TransIrriCost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LMMISTA-WAP803\\Not-so-auto-ml\\train_submission_version.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LMMISTA-WAP803/Not-so-auto-ml/train_submission_version.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Fill the low skewness columns with mean\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LMMISTA-WAP803/Not-so-auto-ml/train_submission_version.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m feat \u001b[39min\u001b[39;00m low_skewed_features:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LMMISTA-WAP803/Not-so-auto-ml/train_submission_version.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     dataset[feat] \u001b[39m=\u001b[39m dataset[feat]\u001b[39m.\u001b[39mfillna(dataset[feat]\u001b[39m.\u001b[39;49mmean())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LMMISTA-WAP803/Not-so-auto-ml/train_submission_version.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     dataset_test[feat] \u001b[39m=\u001b[39m dataset_test[feat]\u001b[39m.\u001b[39mfillna(dataset_test[feat]\u001b[39m.\u001b[39mmean())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LMMISTA-WAP803/Not-so-auto-ml/train_submission_version.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Fill the moderate skewness columns with median\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\pandas\\core\\series.py:6221\u001b[0m, in \u001b[0;36mSeries.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m   6213\u001b[0m \u001b[39m@doc\u001b[39m(make_doc(\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m   6214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean\u001b[39m(\n\u001b[0;32m   6215\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6219\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   6220\u001b[0m ):\n\u001b[1;32m-> 6221\u001b[0m     \u001b[39mreturn\u001b[39;00m NDFrame\u001b[39m.\u001b[39mmean(\u001b[39mself\u001b[39m, axis, skipna, numeric_only, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\pandas\\core\\generic.py:11984\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11977\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean\u001b[39m(\n\u001b[0;32m  11978\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  11979\u001b[0m     axis: Axis \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11982\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  11983\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series \u001b[39m|\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m> 11984\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stat_function(\n\u001b[0;32m  11985\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, nanops\u001b[39m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m  11986\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\pandas\\core\\generic.py:11941\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11937\u001b[0m nv\u001b[39m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[0;32m  11939\u001b[0m validate_bool_kwarg(skipna, \u001b[39m\"\u001b[39m\u001b[39mskipna\u001b[39m\u001b[39m\"\u001b[39m, none_allowed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m> 11941\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reduce(\n\u001b[0;32m  11942\u001b[0m     func, name\u001b[39m=\u001b[39;49mname, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, numeric_only\u001b[39m=\u001b[39;49mnumeric_only\n\u001b[0;32m  11943\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\pandas\\core\\series.py:6129\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   6124\u001b[0m     \u001b[39m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[0;32m   6125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   6126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSeries.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not allow \u001b[39m\u001b[39m{\u001b[39;00mkwd_name\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mnumeric_only\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   6127\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith non-numeric dtypes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   6128\u001b[0m     )\n\u001b[1;32m-> 6129\u001b[0m \u001b[39mreturn\u001b[39;00m op(delegate, skipna\u001b[39m=\u001b[39mskipna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    145\u001b[0m         result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39maxis, skipna\u001b[39m=\u001b[39mskipna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    146\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39maxis, skipna\u001b[39m=\u001b[39mskipna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m datetimelike \u001b[39mand\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     mask \u001b[39m=\u001b[39m isna(values)\n\u001b[1;32m--> 404\u001b[0m result \u001b[39m=\u001b[39m func(values, axis\u001b[39m=\u001b[39maxis, skipna\u001b[39m=\u001b[39mskipna, mask\u001b[39m=\u001b[39mmask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m datetimelike:\n\u001b[0;32m    407\u001b[0m     result \u001b[39m=\u001b[39m _wrap_results(result, orig_values\u001b[39m.\u001b[39mdtype, fill_value\u001b[39m=\u001b[39miNaT)\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\pandas\\core\\nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    716\u001b[0m     dtype_count \u001b[39m=\u001b[39m dtype\n\u001b[0;32m    718\u001b[0m count \u001b[39m=\u001b[39m _get_counts(values\u001b[39m.\u001b[39mshape, mask, axis, dtype\u001b[39m=\u001b[39mdtype_count)\n\u001b[1;32m--> 719\u001b[0m the_sum \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49msum(axis, dtype\u001b[39m=\u001b[39;49mdtype_sum)\n\u001b[0;32m    720\u001b[0m the_sum \u001b[39m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[0;32m    722\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(the_sum, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\LMMISTA-WAP803\\anaconda3\\envs\\mlop\\lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "# Fill the low skewness columns with mean\n",
    "for feat in low_skewed_features:\n",
    "    dataset[feat] = dataset[feat].fillna(dataset[feat].mean())\n",
    "    dataset_test[feat] = dataset_test[feat].fillna(dataset_test[feat].mean())\n",
    "\n",
    "# Fill the moderate skewness columns with median\n",
    "for feat in moderate_skewed_features:\n",
    "    dataset[feat] = dataset[feat].fillna(dataset[feat].median())\n",
    "    dataset_test[feat] = dataset_test[feat].fillna(dataset_test[feat].median())\n",
    "\n",
    "# Fill the high skewness columns with median\n",
    "for feat in high_skewed_features:\n",
    "    dataset[feat] = dataset[feat].fillna(dataset[feat].median())\n",
    "    dataset_test[feat] = dataset_test[feat].fillna(dataset_test[feat].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    if dataset[col].dtype == 'object':\n",
    "        dataset[col] = dataset[col].astype('category').cat.codes\n",
    "    elif dataset[col].dtype == 'datetime64[ns]':\n",
    "        dataset = dataset.drop(columns=[col])\n",
    "\n",
    "for col in dataset_test.columns:\n",
    "    if dataset_test[col].dtype == 'object':\n",
    "        dataset_test[col] = dataset_test[col].astype('category').cat.codes\n",
    "    elif dataset_test[col].dtype == 'datetime64[ns]':\n",
    "        dataset_test = dataset_test.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "y = train_labels    \n",
    "\n",
    "# Train Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=200)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Select the object columns and categorical columns\n",
    "object_cols = X_train.select_dtypes(include=['object'])\n",
    "categorical_cols = X_train.select_dtypes(include=['category'])\n",
    "whole_categorical_cols = pd.concat([object_cols, categorical_cols], axis=1) \n",
    "# generate a list with the name of the categorical columns\n",
    "whole_categorical_cols = whole_categorical_cols.columns\n",
    "# Transfer into a list\n",
    "whole_categorical_cols = list(whole_categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=82)\n",
    "\n",
    "# Prepare an array to store the RMSE for each fold\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "# Initialize an empty array to hold feature importances\n",
    "feature_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Start the K-Fold cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    # Split the data\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Create a LGBMRegressor object\n",
    "    lgbm_model = lgb.LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.01, n_estimators=100)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(\n",
    "        X_train_fold, y_train_fold, \n",
    "        eval_set=[(X_val_fold, y_val_fold)], \n",
    "        eval_metric='mae', \n",
    "        categorical_feature=whole_categorical_cols\n",
    "    )\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_val = lgbm_model.predict(X_val_fold, num_iteration=lgbm_model.best_iteration_)\n",
    "\n",
    "    # Calculate and print RMSE for the current fold\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_val))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    print(f\"Fold {fold}: RMSE: {fold_rmse}\")\n",
    "\n",
    "    # Accumulate feature importances\n",
    "    feature_importances += lgbm_model.feature_importances_\n",
    "\n",
    "    models.append(lgbm_model)\n",
    "\n",
    "# After cross-validation, print the mean RMSE\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores)}, STD RMSE: {np.std(rmse_scores)}\")\n",
    "\n",
    "# Feature importances from all folds\n",
    "feature_importances = feature_importances / n_splits\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    # Make predictions\n",
    "    fold_preds = model.predict(dataset_test, num_iteration=model.best_iteration_)\n",
    "    test_predictions.append(fold_preds)\n",
    "\n",
    "# Now average these predictions\n",
    "test_predictions = np.column_stack(test_predictions)\n",
    "y_pred_test = np.mean(test_predictions, axis=1)\n",
    "\n",
    "# Now you have `y_pred_test` which is the averaged predictions from all folds\n",
    "\n",
    "# Create a submission file\n",
    "dataset_upload = dataset_test_original\n",
    "\n",
    "submission_df = pd.DataFrame({'ID': dataset_upload['ID'], 'Yield': y_pred_test})\n",
    "submission_df.to_csv('submission_07_11_567.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new wandb run\n",
    "wandb.init(project='zindi-crop-challenge', \n",
    "           entity=\"lmu-seminar\")\n",
    "\n",
    "# Define your hyperparameters\n",
    "hyperparameters = dict(\n",
    "    objective='regression',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.009,\n",
    "    n_estimators=120,\n",
    "    # Add other hyperparameters you want to track here\n",
    ")\n",
    "\n",
    "# Save hyperparameters to wandb\n",
    "wandb.config.update(hyperparameters)\n",
    "\n",
    "# Rest of your code for preparing the data\n",
    "# ...\n",
    "\n",
    "# Parameters for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=82)\n",
    "\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "feature_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Start the K-Fold cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Create a LGBMRegressor object with hyperparameters from wandb\n",
    "    lgbm_model = lgb.LGBMRegressor(**wandb.config)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(\n",
    "        X_train_fold, y_train_fold, \n",
    "        eval_set=[(X_val_fold, y_val_fold)], \n",
    "        eval_metric='mae', \n",
    "        categorical_feature=whole_categorical_cols,\n",
    "        callbacks=[wandb.callback()]\n",
    "    )\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_val = lgbm_model.predict(X_val_fold, num_iteration=lgbm_model.best_iteration_)\n",
    "\n",
    "    # Calculate RMSE and log the metric to wandb\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_val))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    wandb.log({'fold_rmse': fold_rmse, 'fold': fold})\n",
    "\n",
    "    # Feature importances\n",
    "    feature_importances += lgbm_model.feature_importances_\n",
    "\n",
    "    # Save the model to wandb\n",
    "    wandb.sklearn.log_model(lgbm_model, 'LGBMRegressor')\n",
    "    \n",
    "    models.append(lgbm_model)\n",
    "\n",
    "# Log mean and standard deviation of RMSE across folds to wandb\n",
    "wandb.log({'mean_rmse': np.mean(rmse_scores), 'std_rmse': np.std(rmse_scores)})\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = feature_importances / n_splits\n",
    "# You can also log feature importances to wandb here\n",
    "\n",
    "# Finalize and close your wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
