{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "# Use ensemble to train the model\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "import sys\n",
    "\n",
    "# In the second part of the notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# This part of importing would be moved to main.py or train.py later\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a hyperparameter class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.objective = 'regression'\n",
    "        self.num_leaves = 31\n",
    "        self.learning_rate = 0.01\n",
    "        self.n_estimators = 120\n",
    "        self.max_bin = 255\n",
    "        self.bagging_fraction = 0.8\n",
    "        self.bagging_freq = 5\n",
    "        self.feature_fraction = 0.8\n",
    "        self.feature_fraction_seed = 9\n",
    "        self.bagging_seed = 9\n",
    "        self.random_state = 42\n",
    "        self.OrgFertilizers = 'None'\n",
    "        self.FirstTopDressFert = 'None' \n",
    "        self.CropbasalFerts = 'None'\n",
    "        self.NursDetFactor = 'None'\n",
    "        self.LandPreparationMethod = 'None'\n",
    "        self.TransDetFactor = 'None'\n",
    "        self.PCropSolidOrgFertAppMethod = 'None'\n",
    "        self.sparse_threshold = 1.0\n",
    "# Instantiate the Hyperparameters class\n",
    "config = Config()\n",
    "\n",
    "# Initialize a wandb run\n",
    "wandb.init(project='zindi-crop-challenge',\n",
    "           entity=\"lmu-seminar\")\n",
    "\n",
    "# Update the wandb configuration with the hyperparameters\n",
    "wandb.config.update(vars(config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "path = 'Train.csv'\n",
    "dataset = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'Test.csv'\n",
    "dataset_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'Yield' column\n",
    "train_labels = dataset['Yield'].copy()\n",
    "dataset = dataset.drop(columns=['ID','Yield'], axis = 1)\n",
    "ID = dataset_test['ID'].copy()\n",
    "dataset_test = dataset_test.drop(columns=['ID'], axis = 1)\n",
    "\n",
    "# Copy the original dataset\n",
    "dataset_original = dataset.copy()\n",
    "dataset_test_original = dataset_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Harv_hand_rent'] = dataset['Harv_hand_rent'].fillna(0)\n",
    "dataset['OrgFertilizers'] = dataset['OrgFertilizers'].fillna(dataset['OrgFertilizers'].mode()[0])\n",
    "dataset['CropbasalFerts'] = dataset['CropbasalFerts'].fillna(dataset['CropbasalFerts'].mode()[0])\n",
    "dataset['FirstTopDressFert'] = dataset['FirstTopDressFert'].fillna(dataset['FirstTopDressFert'].mode()[0])\n",
    "dataset['NursDetFactor'] = dataset['NursDetFactor'].fillna(dataset['NursDetFactor'].mode()[0])\n",
    "dataset['LandPreparationMethod'] = dataset['LandPreparationMethod'].fillna(dataset['LandPreparationMethod'].mode()[0])\n",
    "dataset['TransDetFactor'] = dataset['TransDetFactor'].fillna(dataset['TransDetFactor'].mode()[0])\n",
    "dataset['PCropSolidOrgFertAppMethod'] = dataset['PCropSolidOrgFertAppMethod'].fillna(dataset['PCropSolidOrgFertAppMethod'].mode()[0])\n",
    "\n",
    "dataset_test['Harv_hand_rent'] = dataset_test['Harv_hand_rent'].fillna(0)\n",
    "dataset_test['OrgFertilizers'] = dataset_test['OrgFertilizers'].fillna(dataset_test['OrgFertilizers'].mode()[0])\n",
    "dataset_test['CropbasalFerts'] = dataset_test['CropbasalFerts'].fillna(dataset_test['CropbasalFerts'].mode()[0])\n",
    "dataset_test['FirstTopDressFert'] = dataset_test['FirstTopDressFert'].fillna(dataset_test['FirstTopDressFert'].mode()[0])\n",
    "dataset_test['NursDetFactor'] = dataset_test['NursDetFactor'].fillna(dataset_test['NursDetFactor'].mode()[0])\n",
    "dataset_test['LandPreparationMethod'] = dataset_test['LandPreparationMethod'].fillna(dataset_test['LandPreparationMethod'].mode()[0])\n",
    "dataset_test['TransDetFactor'] = dataset_test['TransDetFactor'].fillna(dataset_test['TransDetFactor'].mode()[0])\n",
    "dataset_test['PCropSolidOrgFertAppMethod'] = dataset_test['PCropSolidOrgFertAppMethod'].fillna(dataset_test['PCropSolidOrgFertAppMethod'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique values for categorical variables to identify sparse classes\n",
    "categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
    "sparse_classes = {col: dataset[col].nunique() for col in categorical_columns if dataset[col].nunique() > 15} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['LandPreparationMethod', 'CropTillageDate', 'RcNursEstDate', 'SeedingSowingTransplanting', 'NursDetFactor', 'TransDetFactor', 'OrgFertilizers', 'CropbasalFerts', 'Harv_date', 'Threshing_date'])\n"
     ]
    }
   ],
   "source": [
    "# check the name of sparse classes\n",
    "print(sparse_classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([43, 78, 61, 62, 125, 155, 31, 34, 107, 162])\n"
     ]
    }
   ],
   "source": [
    "# check the unique values of sparse classes\n",
    "print(sparse_classes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for grouping\n",
    "threshold_percentage = 1\n",
    "threshold = len(dataset) * (threshold_percentage / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group sparse classes\n",
    "def group_sparse_classes(df, column, threshold):\n",
    "    # Find the categories that are below the threshold\n",
    "    value_counts = df[column].value_counts()\n",
    "    to_replace = value_counts[value_counts <= threshold].index.tolist()\n",
    "    \n",
    "    # Replace the sparse classes with 'other'\n",
    "    df[column] = df[column].replace(to_replace, 'other')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grouping for the identified categorical variables with many unique values\n",
    "for col in ['LandPreparationMethod', 'OrgFertilizers', 'CropbasalFerts']:\n",
    "    dataset = group_sparse_classes(dataset, col, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the category adjustment to the test dataset based on the training dataset categories\n",
    "for col in ['LandPreparationMethod', 'OrgFertilizers', 'CropbasalFerts']:\n",
    "    test_dataset = group_sparse_classes(dataset_test, col, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 skewed numerical features to Box Cox transform\n",
      "CultLand                         8.178796\n",
      "CropCultLand                     8.796317\n",
      "SeedlingsPerPit                 53.528364\n",
      "TransplantingIrrigationHours    30.754264\n",
      "TransIrriCost                    3.912326\n",
      "StandingWater                    1.848641\n",
      "Ganaura                          5.237242\n",
      "CropOrgFYM                       6.876849\n",
      "BasalDAP                         2.905715\n",
      "BasalUrea                        2.244465\n",
      "1tdUrea                          2.094479\n",
      "1appDaysUrea                     4.769872\n",
      "2tdUrea                          2.696580\n",
      "2appDaysUrea                    -1.589283\n",
      "Harv_hand_rent                  40.547049\n",
      "Residue_length                  -1.880371\n",
      "Residue_perc                     3.747956\n",
      "Acre                             2.384037\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# fill the high skewness columns with median\n",
    "# check the skewness of the numerical columns\n",
    "skewness = dataset.skew(numeric_only=True)\n",
    "skewness = skewness[abs(skewness) > 0.5]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "# sort the skewness in descending order\n",
    "skewness.sort_values(ascending=False)\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_skewed_features = ['SeedlingsPerPit', 'TransplantingIrrigationHours','Harv_hand_rent']\n",
    "low_skewed_features = ['CultLand', 'CropOrgFYM', 'CropCultLand','Ganaura'] \n",
    "moderate_skewed_features = ['BasalUrea', 'BasalDAP',\n",
    "                            '1tdUrea', '1appDaysUrea', \n",
    "                            '2tdUrea', '2appDaysUrea',\n",
    "                            'Residue_length', 'Residue_perc',\n",
    "                            'StandingWater', 'TransIrriCost',\n",
    "                            'TransIrriCost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeedlingsPerPit                 float64\n",
      "TransplantingIrrigationHours    float64\n",
      "Harv_hand_rent                  float64\n",
      "dtype: object\n",
      "CultLand          int64\n",
      "CropOrgFYM      float64\n",
      "CropCultLand      int64\n",
      "Ganaura         float64\n",
      "dtype: object\n",
      "BasalUrea         float64\n",
      "BasalDAP          float64\n",
      "1tdUrea           float64\n",
      "1appDaysUrea      float64\n",
      "2tdUrea           float64\n",
      "2appDaysUrea      float64\n",
      "Residue_length      int64\n",
      "Residue_perc        int64\n",
      "StandingWater     float64\n",
      "TransIrriCost     float64\n",
      "TransIrriCost     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check the dtype of those skewed features\n",
    "print(dataset[high_skewed_features].dtypes)\n",
    "print(dataset[low_skewed_features].dtypes)\n",
    "print(dataset[moderate_skewed_features].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the low skewness columns with mean\n",
    "for feat in low_skewed_features:\n",
    "    dataset[feat] = dataset[feat].fillna(dataset[feat].mean())\n",
    "    dataset_test[feat] = dataset_test[feat].fillna(dataset_test[feat].mean())\n",
    "\n",
    "# Fill the moderate skewness columns with median\n",
    "for feat in moderate_skewed_features:\n",
    "    dataset[feat] = dataset[feat].fillna(dataset[feat].median())\n",
    "    dataset_test[feat] = dataset_test[feat].fillna(dataset_test[feat].median())\n",
    "\n",
    "# Fill the high skewness columns with median\n",
    "for feat in high_skewed_features:\n",
    "    dataset[feat] = dataset[feat].fillna(dataset[feat].median())\n",
    "    dataset_test[feat] = dataset_test[feat].fillna(dataset_test[feat].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nskewed_features = skewness.index\\nlam = 0.15\\n# apply the boxcox1p transformation to the skewed features\\nfor feat in skewed_features:\\n    dataset[feat] = boxcox1p(dataset[feat], lam)\\n    dataset_test[feat] = boxcox1p(dataset_test[feat], lam)\\n\\n# check the skewness of the numerical columns\\nskewness = dataset.skew(numeric_only=True)\\nskewness = skewness[abs(skewness) > 0.5]\\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "# apply the boxcox1p transformation to the skewed features\n",
    "for feat in skewed_features:\n",
    "    dataset[feat] = boxcox1p(dataset[feat], lam)\n",
    "    dataset_test[feat] = boxcox1p(dataset_test[feat], lam)\n",
    "\n",
    "# check the skewness of the numerical columns\n",
    "skewness = dataset.skew(numeric_only=True)\n",
    "skewness = skewness[abs(skewness) > 0.5]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbins_SeedlingsPerPit = [0, 2, 4, np.inf]\\nlabels_SeedlingsPerPit = ['Low', 'Medium', 'High']\\ndataset['SeedlingsPerPit_Binned'] = pd.cut(dataset['SeedlingsPerPit'], bins=bins_SeedlingsPerPit, labels=labels_SeedlingsPerPit)\\ndataset_test['SeedlingsPerPit_Binned'] = pd.cut(dataset_test['SeedlingsPerPit'], bins=bins_SeedlingsPerPit, labels=labels_SeedlingsPerPit)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bins_SeedlingsPerPit = [0, 2, 4, np.inf]\n",
    "labels_SeedlingsPerPit = ['Low', 'Medium', 'High']\n",
    "dataset['SeedlingsPerPit_Binned'] = pd.cut(dataset['SeedlingsPerPit'], bins=bins_SeedlingsPerPit, labels=labels_SeedlingsPerPit)\n",
    "dataset_test['SeedlingsPerPit_Binned'] = pd.cut(dataset_test['SeedlingsPerPit'], bins=bins_SeedlingsPerPit, labels=labels_SeedlingsPerPit)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbins_NoFertilizerAppln = [0, 1, 2, np.inf]\\nlabels_NoFertilizerAppln = ['Low', 'Medium', 'High']\\ndataset['NoFertilizerAppln_Binned'] = pd.cut(dataset['NoFertilizerAppln'], bins=bins_NoFertilizerAppln, labels=labels_NoFertilizerAppln)\\ndataset_test['NoFertilizerAppln_Binned'] = pd.cut(dataset_test['NoFertilizerAppln'], bins=bins_NoFertilizerAppln, labels=labels_NoFertilizerAppln)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bins_NoFertilizerAppln = [0, 1, 2, np.inf]\n",
    "labels_NoFertilizerAppln = ['Low', 'Medium', 'High']\n",
    "dataset['NoFertilizerAppln_Binned'] = pd.cut(dataset['NoFertilizerAppln'], bins=bins_NoFertilizerAppln, labels=labels_NoFertilizerAppln)\n",
    "dataset_test['NoFertilizerAppln_Binned'] = pd.cut(dataset_test['NoFertilizerAppln'], bins=bins_NoFertilizerAppln, labels=labels_NoFertilizerAppln)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RcNursEstDate is NaN, then fill NursDetFactor with None\n",
    "# if RcNursEstDate is not NaN, then fill NursDetFactor with mode\n",
    "dataset.loc[dataset['RcNursEstDate'].isnull(), 'NursDetFactor'] = 'None'\n",
    "dataset.loc[dataset['RcNursEstDate'].notnull(), 'NursDetFactor'] = dataset['NursDetFactor'].mode()[0]\n",
    "\n",
    "dataset_test.loc[dataset_test['RcNursEstDate'].isnull(), 'NursDetFactor'] = 'None'\n",
    "dataset_test.loc[dataset_test['RcNursEstDate'].notnull(), 'NursDetFactor'] = dataset_test['NursDetFactor'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing value with MineralFertAppMethod.1 with mode\n",
    "dataset['MineralFertAppMethod.1'] = dataset['MineralFertAppMethod.1'].fillna(dataset['MineralFertAppMethod.1'].mode()[0])\n",
    "dataset_test['MineralFertAppMethod.1'] = dataset_test['MineralFertAppMethod.1'].fillna(dataset_test['MineralFertAppMethod.1'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date columns\n",
    "date_columns = ['Harv_date', 'SeedingSowingTransplanting', 'RcNursEstDate', 'Threshing_date', 'CropTillageDate']\n",
    "\n",
    "# fill the missing values with mode\n",
    "for col in date_columns:\n",
    "    dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
    "    dataset_test[col] = dataset_test[col].fillna(dataset_test[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns to datetime\n",
    "for col in date_columns:\n",
    "    dataset[col] = pd.to_datetime(dataset[col])\n",
    "    dataset_test[col] = pd.to_datetime(dataset_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Harv_date_RcNursEstDate'] = (dataset['Harv_date'] - dataset['RcNursEstDate']).dt.days\n",
    "dataset_test['Harv_date_RcNursEstDate'] = (dataset_test['Harv_date'] - dataset_test['RcNursEstDate']).dt.days\n",
    "dataset.loc[dataset['Harv_date_RcNursEstDate'] < 0, 'Harv_date_RcNursEstDate'] = dataset.loc[dataset['Harv_date_RcNursEstDate'] < 0, 'Harv_date_RcNursEstDate'] + 365\n",
    "dataset_test.loc[dataset_test['Harv_date_RcNursEstDate'] < 0, 'Harv_date_RcNursEstDate'] = dataset_test.loc[dataset_test['Harv_date_RcNursEstDate'] < 0, 'Harv_date_RcNursEstDate'] + 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvest_date - CropTillageDate\n",
    "dataset['CropTillageDate'] = pd.to_datetime(dataset['CropTillageDate'])\n",
    "dataset_test['CropTillageDate'] = pd.to_datetime(dataset_test['CropTillageDate'])\n",
    "dataset['Harv_date_CropTillageDate'] = (dataset['Harv_date'] - dataset['CropTillageDate']).dt.days\n",
    "dataset_test['Harv_date_CropTillageDate'] = (dataset_test['Harv_date'] - dataset_test['CropTillageDate']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshing_date - Harvest_date\n",
    "dataset['Threshing_date'] = pd.to_datetime(dataset['Threshing_date'])\n",
    "dataset['Threshing_date_Harv_date'] = (dataset['Threshing_date'] - dataset['Harv_date']).dt.days\n",
    "dataset_test['Threshing_date'] = pd.to_datetime(dataset_test['Threshing_date'])\n",
    "dataset_test['Threshing_date_Harv_date'] = (dataset_test['Threshing_date'] - dataset_test['Harv_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the month features, this will be transfered into one-hot encoding later\n",
    "dataset['Harv_date_Month'] = dataset['Harv_date'].dt.month\n",
    "dataset_test['Harv_date_Month'] = dataset_test['Harv_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the month values to the season values\n",
    "month_to_season = {1: 1, 2: 1, 3: 1, 4: 2, 5: 2,\n",
    "                     6: 2, 7: 3, 8: 3, 9: 3, 10: 4,\n",
    "                     11: 4, 12: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the month values to the season values\n",
    "dataset['Harv_date_Season'] = dataset['Harv_date_Month'].map(month_to_season)\n",
    "dataset_test['Harv_date_Season'] = dataset_test['Harv_date_Month'].map(month_to_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['Harv_date_Month'], axis=1)\n",
    "dataset_test = dataset_test.drop(columns=['Harv_date_Month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original date columns\n",
    "dataset = dataset.drop(columns=date_columns, axis=1)\n",
    "dataset_test = dataset_test.drop(columns=date_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Org_Crop'] = dataset['OrgFertilizers'].astype(str) + \" + \" + dataset['CropbasalFerts'].astype(str)\n",
    "dataset_test['Org_Crop'] = dataset_test['OrgFertilizers'].astype(str) + \"+\" + dataset_test['CropbasalFerts'].astype(str)\n",
    "dataset['Crop_FirstTop'] = dataset['CropbasalFerts'].astype(str) + \"+\" + dataset['FirstTopDressFert'].astype(str)\n",
    "dataset_test['Crop_FirstTop'] = dataset_test['CropbasalFerts'].astype(str) + \"+\" + dataset_test['FirstTopDressFert'].astype(str)\n",
    "dataset['Org_FirstTop'] = dataset['OrgFertilizers'].astype(str) + \"+\" + dataset['FirstTopDressFert'].astype(str)\n",
    "dataset_test['Org_FirstTop'] = dataset_test['OrgFertilizers'].astype(str) + \"+\" + dataset_test['FirstTopDressFert'].astype(str)\n",
    "dataset['PCropSolidOrgFertAppMethod'] = dataset['PCropSolidOrgFertAppMethod'].fillna('other')\n",
    "dataset_test['PCropSolidOrgFertAppMethod'] = dataset_test['PCropSolidOrgFertAppMethod'].fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all those new columns\n",
    "dataset = dataset.drop(columns=['Org_Crop', 'Crop_FirstTop', 'Org_FirstTop', 'PCropSolidOrgFertAppMethod'], axis=1)\n",
    "dataset_test = dataset_test.drop(columns=['Org_Crop', 'Crop_FirstTop', 'Org_FirstTop', 'PCropSolidOrgFertAppMethod'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    if dataset[col].dtype == 'object':\n",
    "        dataset[col] = dataset[col].astype('category').cat.codes\n",
    "\n",
    "for col in dataset_test.columns:\n",
    "    if dataset_test[col].dtype == 'object':\n",
    "        dataset_test[col] = dataset_test[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "y = train_labels    \n",
    "\n",
    "# Train Test split\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=200)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Select the object columns and categorical columns\n",
    "object_cols = X.select_dtypes(include=['object'])\n",
    "categorical_cols = X.select_dtypes(include=['category'])\n",
    "whole_categorical_cols = pd.concat([object_cols, categorical_cols], axis=1) \n",
    "# generate a list with the name of the categorical columns\n",
    "whole_categorical_cols = whole_categorical_cols.columns\n",
    "# Transfer into a list\n",
    "whole_categorical_cols = list(whole_categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.581310\n",
      "Fold 0: RMSE: 319.958615204726\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.968155\n",
      "Fold 1: RMSE: 340.9366266161907\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.912816\n",
      "Fold 2: RMSE: 218.07072031897886\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.309319\n",
      "Fold 3: RMSE: 213.1735756569988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.335683\n",
      "Fold 4: RMSE: 196.29116405013005\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.186635\n",
      "Fold 5: RMSE: 204.68165020354587\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.217698\n",
      "Fold 6: RMSE: 238.93791409137273\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1253\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.832681\n",
      "Fold 7: RMSE: 261.77081540143615\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.784652\n",
      "Fold 8: RMSE: 589.1642007962452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 596.275907\n",
      "Fold 9: RMSE: 174.67495089390366\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.651788\n",
      "Fold 10: RMSE: 898.2183669832259\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.466980\n",
      "Fold 11: RMSE: 265.47021989365686\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.617854\n",
      "Fold 12: RMSE: 261.7162786709502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.458105\n",
      "Fold 13: RMSE: 157.0329381944073\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.847037\n",
      "Fold 14: RMSE: 230.2462619202882\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.641869\n",
      "Fold 15: RMSE: 1151.3313760809913\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.785957\n",
      "Fold 16: RMSE: 380.29890898195777\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.540329\n",
      "Fold 17: RMSE: 269.18594410826057\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.696424\n",
      "Fold 18: RMSE: 164.9141328525679\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.501436\n",
      "Fold 19: RMSE: 426.81016931088163\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.073610\n",
      "Fold 20: RMSE: 215.57418840499653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.653876\n",
      "Fold 21: RMSE: 779.8499433379247\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.897938\n",
      "Fold 22: RMSE: 216.8433648242036\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.796920\n",
      "Fold 23: RMSE: 146.60436585457262\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.195249\n",
      "Fold 24: RMSE: 197.79875413094865\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 591.491256\n",
      "Fold 25: RMSE: 2432.7109174607544\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.136779\n",
      "Fold 26: RMSE: 148.42378472102388\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1253\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.879144\n",
      "Fold 27: RMSE: 1123.2131929822963\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.508222\n",
      "Fold 28: RMSE: 803.7532013380313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.147220\n",
      "Fold 29: RMSE: 184.62668085880563\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.342469\n",
      "Fold 30: RMSE: 768.9830463957704\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.347168\n",
      "Fold 31: RMSE: 433.059728424273\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.347690\n",
      "Fold 32: RMSE: 153.86944522396044\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.026625\n",
      "Fold 33: RMSE: 146.15722144731112\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.673975\n",
      "Fold 34: RMSE: 149.3289819446574\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.971548\n",
      "Fold 35: RMSE: 369.2330683857945\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.116941\n",
      "Fold 36: RMSE: 185.08788219964183\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.890107\n",
      "Fold 37: RMSE: 193.27928650424602\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1253\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.113547\n",
      "Fold 38: RMSE: 254.79061064295328\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.391021\n",
      "Fold 39: RMSE: 238.8075773151453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.393109\n",
      "Fold 40: RMSE: 218.36817303355417\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.404333\n",
      "Fold 41: RMSE: 191.1425205956132\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.675020\n",
      "Fold 42: RMSE: 152.2400781306465\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.795615\n",
      "Fold 43: RMSE: 208.35982739159638\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.622814\n",
      "Fold 44: RMSE: 262.7536166108104\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.109893\n",
      "Fold 45: RMSE: 206.68069016192933\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.475594\n",
      "Fold 46: RMSE: 241.36568360706917\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.491517\n",
      "Fold 47: RMSE: 212.43417877105472\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1253\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.404072\n",
      "Fold 48: RMSE: 232.91300793411446\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.727747\n",
      "Fold 49: RMSE: 387.05912848304376\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.684678\n",
      "Fold 50: RMSE: 156.75661062527365\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.095014\n",
      "Fold 51: RMSE: 216.94058445712042\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 596.060559\n",
      "Fold 52: RMSE: 159.26282601766084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.474811\n",
      "Fold 53: RMSE: 1153.6210331869604\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.733490\n",
      "Fold 54: RMSE: 173.70008612689855\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.736100\n",
      "Fold 55: RMSE: 213.42978194300463\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.987993\n",
      "Fold 56: RMSE: 238.01124586231947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.327591\n",
      "Fold 57: RMSE: 2119.1763166069536\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.034978\n",
      "Fold 58: RMSE: 318.185549057561\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.701906\n",
      "Fold 59: RMSE: 183.60774534979666\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.444270\n",
      "Fold 60: RMSE: 365.1548518806583\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.612112\n",
      "Fold 61: RMSE: 218.69419743603441\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.212477\n",
      "Fold 62: RMSE: 180.1423825657645\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.296267\n",
      "Fold 63: RMSE: 312.1596753910964\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.353955\n",
      "Fold 64: RMSE: 577.392487556158\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.471678\n",
      "Fold 65: RMSE: 167.50368284000044\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.680762\n",
      "Fold 66: RMSE: 155.24566269651797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.091099\n",
      "Fold 67: RMSE: 187.73267470562044\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.938658\n",
      "Fold 68: RMSE: 232.95163744312873\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 3831, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.753067\n",
      "Fold 69: RMSE: 257.6726320105845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.289405\n",
      "Fold 70: RMSE: 1107.8725463640508\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.852557\n",
      "Fold 71: RMSE: 210.62002127882562\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.221294\n",
      "Fold 72: RMSE: 189.06442375641288\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.068894\n",
      "Fold 73: RMSE: 167.05174401742917\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.822547\n",
      "Fold 74: RMSE: 516.1685313642464\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.246086\n",
      "Fold 75: RMSE: 293.91149734050487\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.474426\n",
      "Fold 76: RMSE: 137.8724974752686\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.030532\n",
      "Fold 77: RMSE: 209.62420637396576\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.182150\n",
      "Fold 78: RMSE: 165.1397168155788\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.101775\n",
      "Fold 79: RMSE: 172.1575477290052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1259\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.731733\n",
      "Fold 80: RMSE: 219.04493601497128\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.490344\n",
      "Fold 81: RMSE: 161.27515806882488\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.950418\n",
      "Fold 82: RMSE: 209.23774538480046\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.538883\n",
      "Fold 83: RMSE: 650.6329665574822\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.051931\n",
      "Fold 84: RMSE: 678.8652601305175\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1256\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.808194\n",
      "Fold 85: RMSE: 141.01410728899262\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.125522\n",
      "Fold 86: RMSE: 152.35257264520797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.202505\n",
      "Fold 87: RMSE: 159.58233220674234\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 595.594729\n",
      "Fold 88: RMSE: 187.34844296969746\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.154228\n",
      "Fold 89: RMSE: 180.93719933748625\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1257\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.533925\n",
      "Fold 90: RMSE: 209.31806249141152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.123695\n",
      "Fold 91: RMSE: 200.28999469625612\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.836900\n",
      "Fold 92: RMSE: 206.085788306978\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.094207\n",
      "Fold 93: RMSE: 198.937410017553\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.381524\n",
      "Fold 94: RMSE: 249.76557372153772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 592.909447\n",
      "Fold 95: RMSE: 887.5652191346836\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1258\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.686065\n",
      "Fold 96: RMSE: 289.87253086566636\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1253\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.724948\n",
      "Fold 97: RMSE: 327.64743857663916\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 594.585856\n",
      "Fold 98: RMSE: 323.8278389523075\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1255\n",
      "[LightGBM] [Info] Number of data points in the train set: 3832, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 593.704332\n",
      "Fold 99: RMSE: 285.5451757637992\n",
      "Mean RMSE: 350.94167126753416\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_splits = 100\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=90)\n",
    "\n",
    "# Prepare an array to store the RMSE for each fold\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "# Initialize an empty array to hold feature importances\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "# Start the K-Fold cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    # Split the data\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Create a LGBMRegressor object\n",
    "    lgbm_model = lgb.LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.01, n_estimators=100)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(\n",
    "        X_train_fold, y_train_fold, \n",
    "        eval_set=[(X_val_fold, y_val_fold)], \n",
    "        eval_metric='mae', \n",
    "        categorical_feature=whole_categorical_cols\n",
    "    )\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_val = lgbm_model.predict(X_val_fold, num_iteration=0)\n",
    "\n",
    "    # Calculate and print RMSE for the current fold\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_val))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    print(f\"Fold {fold}: RMSE: {fold_rmse}\")\n",
    "\n",
    "    # Accumulate feature importances\n",
    "    feature_importances += lgbm_model.feature_importances_\n",
    "\n",
    "    models.append(lgbm_model)\n",
    "\n",
    "# After cross-validation, print the mean RMSE\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores)}\")\n",
    "\n",
    "# Feature importances from all folds\n",
    "feature_importances = feature_importances / n_splits\n",
    "# print(feature_importances)\n",
    "\n",
    "# Create a dataframe of feature importances\n",
    "# feature_importances_df = pd.DataFrame({'feature': list(X_train.columns), 'importance': feature_importances}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Print the top 20 features with their importance values\n",
    "# print(feature_importances_df.head(20))\n",
    "test_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    # Make predictions\n",
    "    fold_preds = model.predict(dataset_test, num_iteration=model.best_iteration_)\n",
    "    test_predictions.append(fold_preds)\n",
    "\n",
    "# Average these predictions\n",
    "test_predictions = np.column_stack(test_predictions)\n",
    "y_pred_test = np.mean(test_predictions, axis=1)\n",
    "\n",
    "submission_df = pd.DataFrame({'ID': ID, 'Yield': y_pred_test})\n",
    "submission_df.to_csv('result_file/submission_11_22_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(dataset_test.columns) - set(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new wandb run\n",
    "wandb.init(project='zindi-crop-challenge', \n",
    "           entity=\"lmu-seminar\")\n",
    "\n",
    "# Define your hyperparameters\n",
    "hyperparameters = dict(\n",
    "    objective='regression',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.009,\n",
    "    n_estimators=120,\n",
    "    # Add other hyperparameters you want to track here\n",
    ")\n",
    "\n",
    "# Save hyperparameters to wandb\n",
    "wandb.config.update(hyperparameters)\n",
    "\n",
    "# Rest of your code for preparing the data\n",
    "# ...\n",
    "\n",
    "# Parameters for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=82)\n",
    "\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "feature_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Start the K-Fold cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Create a LGBMRegressor object with hyperparameters from wandb\n",
    "    lgbm_model = lgb.LGBMRegressor(**wandb.config)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(\n",
    "        X_train_fold, y_train_fold, \n",
    "        eval_set=[(X_val_fold, y_val_fold)], \n",
    "        eval_metric='mae', \n",
    "        categorical_feature=whole_categorical_cols,\n",
    "        callbacks=[wandb.callback()]\n",
    "    )\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_val = lgbm_model.predict(X_val_fold, num_iteration=lgbm_model.best_iteration_)\n",
    "\n",
    "    # Calculate RMSE and log the metric to wandb\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_val))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    wandb.log({'fold_rmse': fold_rmse, 'fold': fold})\n",
    "\n",
    "    # Feature importances\n",
    "    feature_importances += lgbm_model.feature_importances_\n",
    "\n",
    "    # Save the model to wandb\n",
    "    wandb.sklearn.log_model(lgbm_model, 'LGBMRegressor')\n",
    "    \n",
    "    models.append(lgbm_model)\n",
    "\n",
    "# Log mean and standard deviation of RMSE across folds to wandb\n",
    "wandb.log({'mean_rmse': np.mean(rmse_scores), 'std_rmse': np.std(rmse_scores)})\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = feature_importances / n_splits\n",
    "# You can also log feature importances to wandb here\n",
    "\n",
    "# Finalize and close your wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
