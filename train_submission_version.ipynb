{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "# Use ensemble to train the model\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "import sys\n",
    "\n",
    "# In the second part of the notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# This part of importing would be moved to main.py or train.py later\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a hyperparameter class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.objective = 'regression'\n",
    "        self.num_leaves = 31\n",
    "        self.learning_rate = 0.01\n",
    "        self.n_estimators = 120\n",
    "        self.max_bin = 255\n",
    "        self.bagging_fraction = 0.8\n",
    "        self.bagging_freq = 5\n",
    "        self.feature_fraction = 0.8\n",
    "        self.feature_fraction_seed = 9\n",
    "        self.bagging_seed = 9\n",
    "        self.random_state = 42\n",
    "        self.OrgFertilizers = 'None'\n",
    "        self.FirstTopDressFert = 'None' \n",
    "        self.CropbasalFerts = 'None'\n",
    "        self.NursDetFactor = 'None'\n",
    "        self.LandPreparationMethod = 'None'\n",
    "        self.TransDetFactor = 'None'\n",
    "        self.PCropSolidOrgFertAppMethod = 'None'\n",
    "        self.sparse_threshold = 1.0\n",
    "# Instantiate the Hyperparameters class\n",
    "config = Config()\n",
    "\n",
    "# Initialize a wandb run\n",
    "wandb.init(project='zindi-crop-challenge',\n",
    "           entity=\"lmu-seminar\")\n",
    "\n",
    "# Update the wandb configuration with the hyperparameters\n",
    "wandb.config.update(vars(config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new wandb run\n",
    "wandb.init(project='zindi-crop-challenge', \n",
    "           entity=\"lmu-seminar\")\n",
    "\n",
    "# Define your hyperparameters\n",
    "hyperparameters = dict(\n",
    "    objective='regression',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.009,\n",
    "    n_estimators=120,\n",
    "    # Add other hyperparameters you want to track here\n",
    ")\n",
    "\n",
    "# Save hyperparameters to wandb\n",
    "wandb.config.update(hyperparameters)\n",
    "\n",
    "# Rest of your code for preparing the data\n",
    "# ...\n",
    "\n",
    "# Parameters for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=82)\n",
    "\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "feature_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Start the K-Fold cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Create a LGBMRegressor object with hyperparameters from wandb\n",
    "    lgbm_model = lgb.LGBMRegressor(**wandb.config)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(\n",
    "        X_train_fold, y_train_fold, \n",
    "        eval_set=[(X_val_fold, y_val_fold)], \n",
    "        eval_metric='mae', \n",
    "        categorical_feature=whole_categorical_cols,\n",
    "        callbacks=[wandb.callback()]\n",
    "    )\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_val = lgbm_model.predict(X_val_fold, num_iteration=lgbm_model.best_iteration_)\n",
    "\n",
    "    # Calculate RMSE and log the metric to wandb\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_val))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    wandb.log({'fold_rmse': fold_rmse, 'fold': fold})\n",
    "\n",
    "    # Feature importances\n",
    "    feature_importances += lgbm_model.feature_importances_\n",
    "\n",
    "    # Save the model to wandb\n",
    "    wandb.sklearn.log_model(lgbm_model, 'LGBMRegressor')\n",
    "    \n",
    "    models.append(lgbm_model)\n",
    "\n",
    "# Log mean and standard deviation of RMSE across folds to wandb\n",
    "wandb.log({'mean_rmse': np.mean(rmse_scores), 'std_rmse': np.std(rmse_scores)})\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = feature_importances / n_splits\n",
    "# You can also log feature importances to wandb here\n",
    "\n",
    "# Finalize and close your wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    objective = 'regression'\n",
    "    num_leaves = 31\n",
    "    learning_rate = 0.009\n",
    "    n_estimators = 120\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "path = r'C:\\Users\\ra78lof\\Not-so-auto-ml\\Train.csv'\n",
    "dataset = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r'C:\\Users\\ra78lof\\Not-so-auto-ml\\Test.csv'\n",
    "dataset_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'Yield' column\n",
    "train_labels = dataset['Yield'].copy()\n",
    "dataset = dataset.drop(columns=['Yield'])\n",
    "\n",
    "# Copy the original dataset\n",
    "dataset_original = dataset.copy()\n",
    "dataset_test_original = dataset_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    if dataset[col].dtype == 'object':\n",
    "        dataset[col] = dataset[col].astype('category').cat.codes\n",
    "    elif dataset[col].dtype == 'datetime64[ns]':\n",
    "        dataset = dataset.drop(columns=[col])\n",
    "\n",
    "for col in dataset_test.columns:\n",
    "    if dataset_test[col].dtype == 'object':\n",
    "        dataset_test[col] = dataset_test[col].astype('category').cat.codes\n",
    "    elif dataset_test[col].dtype == 'datetime64[ns]':\n",
    "        dataset_test = dataset_test.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "y = train_labels    \n",
    "\n",
    "# Train Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=200)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Select the object columns and categorical columns\n",
    "object_cols = X_train.select_dtypes(include=['object'])\n",
    "categorical_cols = X_train.select_dtypes(include=['category'])\n",
    "whole_categorical_cols = pd.concat([object_cols, categorical_cols], axis=1) \n",
    "# generate a list with the name of the categorical columns\n",
    "whole_categorical_cols = whole_categorical_cols.columns\n",
    "# Transfer into a list\n",
    "whole_categorical_cols = list(whole_categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1658\n",
      "[LightGBM] [Info] Number of data points in the train set: 2786, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 599.349605\n",
      "Fold 0: RMSE: 347.1344370327278\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1683\n",
      "[LightGBM] [Info] Number of data points in the train set: 2786, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 594.891960\n",
      "Fold 1: RMSE: 412.0191218418146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1666\n",
      "[LightGBM] [Info] Number of data points in the train set: 2786, number of used features: 43\n",
      "[LightGBM] [Info] Start training from score 593.094042\n",
      "Fold 2: RMSE: 438.5975491625796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1659\n",
      "[LightGBM] [Info] Number of data points in the train set: 2787, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 606.564406\n",
      "Fold 3: RMSE: 227.18992738442122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1672\n",
      "[LightGBM] [Info] Number of data points in the train set: 2787, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 591.085755\n",
      "Fold 4: RMSE: 717.7510772206092\n",
      "Mean RMSE: 428.53842252843043, STD RMSE: 161.97892351121226\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=82)\n",
    "\n",
    "# Prepare an array to store the RMSE for each fold\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "# Initialize an empty array to hold feature importances\n",
    "feature_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Start the K-Fold cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    # Split the data\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Create a LGBMRegressor object\n",
    "    lgbm_model = lgb.LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.01, n_estimators=100)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(\n",
    "        X_train_fold, y_train_fold, \n",
    "        eval_set=[(X_val_fold, y_val_fold)], \n",
    "        eval_metric='mae', \n",
    "        categorical_feature=whole_categorical_cols\n",
    "    )\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred_val = lgbm_model.predict(X_val_fold, num_iteration=lgbm_model.best_iteration_)\n",
    "\n",
    "    # Calculate and print RMSE for the current fold\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_val))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    print(f\"Fold {fold}: RMSE: {fold_rmse}\")\n",
    "\n",
    "    # Accumulate feature importances\n",
    "    feature_importances += lgbm_model.feature_importances_\n",
    "\n",
    "    models.append(lgbm_model)\n",
    "\n",
    "# After cross-validation, print the mean RMSE\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores)}, STD RMSE: {np.std(rmse_scores)}\")\n",
    "\n",
    "# Feature importances from all folds\n",
    "feature_importances = feature_importances / n_splits\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    # Make predictions\n",
    "    fold_preds = model.predict(dataset_test, num_iteration=model.best_iteration_)\n",
    "    test_predictions.append(fold_preds)\n",
    "\n",
    "# Now average these predictions\n",
    "test_predictions = np.column_stack(test_predictions)\n",
    "y_pred_test = np.mean(test_predictions, axis=1)\n",
    "\n",
    "# Now you have `y_pred_test` which is the averaged predictions from all folds\n",
    "\n",
    "# Create a submission file\n",
    "dataset_upload = dataset_test_original\n",
    "\n",
    "submission_df = pd.DataFrame({'ID': dataset_upload['ID'], 'Yield': y_pred_test})\n",
    "submission_df.to_csv('submission_07_11_567.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
